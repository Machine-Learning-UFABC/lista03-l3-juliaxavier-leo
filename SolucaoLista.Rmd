---
title: "Solução Lista 02"
author: |
        | Nome: Julia Xavier
        | E-mail: julia.xavier@aluno.ufabc.edu.br
        | Nome: Leonardo Bernardes Lerio da Silva
        | E-mail: leonardo.lerio@aluno.ufabc.edu.br
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      fig.align='center',
                      cache=TRUE,
                      out.width = "60%",
                      out.heigth = "60%",
                      warning=FALSE,
                      message=FALSE)
options(width =70)

library(reticulate)
use_python('c:/Users/leole/AppData/Local/Microsoft/WindowsApps/python3.11.exe')
```

## Exercício 01
```{python}
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
```

semente para os resultados serem sempre os mesmos
```{python}
np.random.seed(1234)
```

treino
```{python}
x_train = np.random.uniform(-1, 1, size=(50,))
eq_train = x_train**2 - 2 * x_train**3 + 1
y_train = eq_train + np.random.normal(0, 0.5, size=(50,))
```
teste
```{python}
x_test = np.random.uniform(-1, 1, size=(100,))
eq_test = x_test**2 - 2 * x_test**3 + 1
y_test = eq_test + np.random.normal(0, 0.5, size=(100,))
```

```{python}
train_data = pd.DataFrame({'x': x_train, 'y': y_train})
test_data = pd.DataFrame({'x': x_test, 'y': y_test})
```

extrai variaveis explicativas e resposta
```{python}
x_train_rl = train_data[['x']]
y_train_rl = train_data['y']
```

gera modelo de regressão linear
```{python}
rl = LinearRegression()
rl.fit(x_train_rl, y_train_rl)

y_pred_rl = rl.predict(test_data[['x']])
```

calcula o erro médio quadrado da regressão linear
```{python}
erro_rl = mean_squared_error(y_test, y_pred_rl)
```

cria o modelo kNN com k = 1
```{python}
knn_1 = KNeighborsRegressor(n_neighbors=1)
knn_1.fit(x_train_rl, y_train_rl)

y_pred_knn_1 = knn_1.predict(test_data[['x']])
```

calcula o erro médio quadrado do  kNN = 1
```{python}
erro_knn_1 = mean_squared_error(y_test, y_pred_knn_1)
```

cria o modelo kNN com k = 10
```{python}
knn_10 = KNeighborsRegressor(n_neighbors=10)
knn_10.fit(x_train_rl, y_train_rl)

y_pred_knn_10 = knn_10.predict(test_data[['x']])
```

calcula o erro médio quadrado com kNN = 10
```{python}
erro_knn_10 = mean_squared_error(y_test, y_pred_knn_10)
```

gera os valores dos erros
```{python}
print("Erro do modelo Regressão Linear: ", erro_rl)
print("Erro do modelo kNN com k=1: ", erro_knn_1)
print("Erro do modelo kNN com k=10: ", erro_knn_10)
```

resultado final
```{python}
if erro_rl < erro_knn_1 and erro_rl < erro_knn_10:
    print("A rl tem menor erro, tem baixo viés, mas pode ter alta variância")
elif erro_knn_1 < erro_rl and erro_knn_1 < erro_knn_10:
    print("O kNN com k=1 tem o menor erro, tem baixo viés, mas pode ter alta variância")
else:
    print("O kNN com k=10 tem o menor erro, tem baixo viés, mas pode ter alta variância")
```
## Exercício 02
```{python}
import pandas as pd
import seaborn as sns
```

```{python}
data = pd.read_csv('diabetes.csv')
```

```{python}
data.head()
```

```{python}
resposta = 'Outcome'
```

```{python}
sns.pairplot(data, hue = resposta)
```

```{python}
sns.pairplot(data, hue = resposta)
```
## Exercício 03
```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
```

Leitura dos dados
```{python}
data = pd.read_csv('diabetes.csv')
data.head()
```
Separação dos dados e dos resultados (X e Y)
```{python}
X = data[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]
Y = data['Outcome']
```

Separação para treinamento e teste (0.2 de teste e 0.8 de treinamento)
```{python}
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
```

Classificação por KNN usando numeros de vizinhos de 1 a 49
```{python}
max = 0
max_n = 0
for x in range(1,50):
  knn = KNeighborsClassifier(n_neighbors=x)
  knn.fit(X_train, y_train)
  y_pred = knn.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  print("Accuracy:", accuracy)
  if accuracy > max:
    max = accuracy
    max_n = x
```

Resultado do melhor numero de vizinhos
```{python}
print("Max Accuracy:", max)
print("Melhor número de vizinhos:", max_n)
```

## Exercício 04
```{python}
import pandas as pd
from sklearn.model_selection import cross_val_score, KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from scipy.stats import sem
import matplotlib.pyplot as plt
import numpy as np
```

Leitura dos dados
```{python}
data = pd.read_csv('diabetes.csv')
data.head()
```

Separação X e Y
```{python}
X = data[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]
Y = data['Outcome']
```

Classficação com validação cruzada de 10 folds e de k entre 1 e 50
```{python}
kfold = KFold(n_splits=10, shuffle=True, random_state=42)

errors = []
k_values = np.arange(1, 50, 2)

min = 999
best_k = 0
for k in k_values:
  knn = KNeighborsClassifier(n_neighbors=k)
  scores = cross_val_score(knn, X, Y, cv=kfold)
  standard_error = sem(scores)
  print("Standard Error:", standard_error)
  errors.append(standard_error)
  if standard_error < min:
    min = standard_error
    best_k = k
```

Melhor K
```{python}
print("Min error:", min)
print("Melhor número de vizinhos:", best_k)
```

Tabela
```{python}
plt.plot(1 / k_values, errors, marker='o')
plt.xlabel('1/k')
plt.ylabel('Erro de Classificação')
plt.title('Erro vs 1/k')
plt.grid(True)
plt.show()
```
